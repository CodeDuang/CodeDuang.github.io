# [实习回忆录]北大成都研究所_万物纵横_西门子

## 北大成都研究所
*（24年7月-9月）*
<br></br>
&emsp;&emsp;作为第一个真正意义上的实习，让我形成了对实习环境、实习内容好坏的标准，
在研究所的实习，自由的同时也会有些漫无目的，每天就是看论文，然后在顶刊顶会上找一些idea实现，
因为是在生物为主的课题组实习，其对计算机领域的认识相对比较理想，所以常常对提出的需求保有很高的期望，
且希望能在短时间内成为成熟产品，导致第一个月做的两个产品，最终都因为没有达到需求，而切掉没有后续了。  
且新成立的课题组，人数很少，所以希望实习生能独当一面，后续和华西一起合作的项目，仅派了我一人完成，
由于没有相关的生物理论背景，且华西的项目环境要求比较高，在没有网+没有显卡的情况去研究人工智能，
进度极慢，在实习期结束时，只完成了几个机器学习模型的尝试，最终遗憾结束本次实习。  
&emsp;&emsp;吐槽归吐槽，但确实自己水平低，尤其是计算机理论基础很差，对于损失函数，从不计较其是怎么计算的，
只关注loss曲线是否先下降，然后慢慢趋于稳定。大部分效果差的原因，总是靠猜测，而没有实际的数据和严谨的理论支撑。
在实习期间，完完全全的丧失了跟进科技前沿的动力，每天都是试图在生物和ai交叉领域（生物为主）找到一些新思路，
但实际上，交叉领域所使用的ai技术，要么是很早以前的成熟技术，要么是已经被筛选过一次的生物本土化严重的工程技术，
这很致命，尤其是在下一段实习期间体验尤其深刻，请务必要保持查看最新github开源项目和最新科技进展的习惯，
也要随时做好替换手上用的很习惯的工具的准备（如最近在使用python uv替换conda，性能是真高，但陌生的命令学起来也很难受）。  
&emsp;&emsp;实习期间收获了什么呢？最大的收获，是对高学历的去魅，对找实习的去怯。不会因为谁是C2毕业的学生，而敬若神明，
把其说的话奉为圭臬。但也让我对这段实习的工作环境非常向往，每个月都有happly hour（类似于茶歇），上班时间早9晚5点半，
中午休息2小时，常备小零食，工学椅很舒服。


## 万物纵横
*（24年10月-25年5月）*
<br></br> 
&emsp;&emsp;可以明显感觉到有实习经历后，更容易进面。但直到25年2月，始终无法理解，
实习中项目经历除了表示自己用过很多工具，还有什么作用，也不太理解队伍配合之类的，
总觉得队友只会拖慢项目节奏。  
&emsp;&emsp;刚进公司，主要任务是使用yolov8训练模型，通过yolov8作为跳板，作为nlp方向的学生，
窥得cv的冰山一角；通过进一步对yolov8的了解，发现yolov8的强大，体现在能以几乎同一个结构完成：
目标检测、目标分割、图像分类等多个任务，且在后续项目中，发现yolov8针对与小目标检测，
还专门有个yolov8-P2用于提高小目标检测性能（其实yolov8之所以在小目标检测性能不佳，在于其有3个检测头，
每个检测头用于检测特定大小以上的目标，最小的为8*8）。  
&emsp;&emsp;除了模型的训练，还有一个任务：把GPU服务器上训练出来的pt模型(torch)，
转换成能在ai盒子上运行的bmodel模型，这个任务是配合算能公司来完成，虽然不是自己研究一个方案
（我的水平也不可能研究得出来），但从中也学到了很多，包括公共模型格式onnx，以及如何与合作公司的员工进行交流。  
&emsp;&emsp;我发现，要追逐最前沿的新技术（比如边缘ai），那你使用的新平台、开发用的新工具，有可能和你所在的公司一样，
也在不断开发自己的新平台、新工具，导致的结果就是：你会遇到不少问题，包括文档缺失、使用的平台环境自我冲突、
工具使用后结果和描述的不同，这个就需要两边公司的开发相互交流，在交流的时候补充对新平台、新工具的更多的认识，
甚至于作为一个bug，要等到新平台的下一个版本才能解决。究其原因，想要作为先驱在荒野上探索新道路，
就要在基本完成功能后火速发布占领市场，没有前人的经验自然会出现很多问题，这些问题需要和合作公司一起发现并解决，
然后作为经验反哺自身，才会有未来完善方便好用的产品。（所以自然，在探索的路上碰壁很常见）
（用了华为的昇腾服务器来训练运行模型，只能说，现在的昇腾真难用）  
&emsp;&emsp;最近开始和同事一起配合开发软件，发现只要规定好自己负责的部分，以及两方之间交互的标准（一般由调用方写一个接口类，
并规定好需要哪些函数，以及参数和返回，交给被调用方，来填充这个接口类，使之完整），就可以做到合作开发，这边以3人开发app作为一个例子，
app需要有数据库、特定功能、页面、高并发or同步or异步，由A完成数据库和特定功能的封装，B调用A封装的类or函数实现并发，并再次封装，C完成页面开发，
然后调用B封装的成熟接口，最终完成app开发。  
&emsp;&emsp;一些开发心得：假如你使用的工具也在不断更新，请在跑通时务必把所有代码和环境在本地备份一份，否则，过段时间，可能原本跑通的项目因为工具的更新无法运行了。  
<br/>
<br/>
&emsp;&emsp;遇到的一些问题和思考：某一天，需要对比测试yolov8在GPU和CPU上的推理速度，代码和结果如下(关于GPU第一张图inference, 372.8ms过于离谱):

```python
import torch
from ultralytics import YOLO
import time

start = time.time()

if True:
    device = torch.device("cuda:4")  # 使用第 0 块 GPU
    print("Using GPU for inference")
else:
    device = torch.device("cpu")
    print("Using CPU for inference")

# 加载模型并指定设备
model = YOLO('/home/xcl/datasets/face_detect/code/runs/detect/train/weights/best.pt')
model.to(device)

# 推理
model('/home/xcl/datasets/face_detect/a.jpg')
model('/home/xcl/datasets/face_detect/b.jpg')
model('/home/xcl/datasets/face_detect/c.png')
model('/home/xcl/datasets/face_detect/d.png')
model('/home/xcl/datasets/face_detect/e.png')
model('/home/xcl/datasets/face_detect/f.png')


end = time.time()
print("模型预测时间：%.4f"%(end-start))

'''GPU
Using GPU for inference

image 1/1 /home/xcl/datasets/face_detect/a.jpg: 384x640 6 faces, 31.6ms
Speed: 3.4ms preprocess, 31.6ms inference, 372.8ms postprocess per image at shape (1, 3, 384, 640)

image 1/1 /home/xcl/datasets/face_detect/b.jpg: 384x640 13 faces, 6.3ms
Speed: 1.4ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)

image 1/1 /home/xcl/datasets/face_detect/c.png: 512x640 25 faces, 30.0ms
Speed: 1.4ms preprocess, 30.0ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)

image 1/1 /home/xcl/datasets/face_detect/d.png: 640x576 24 faces, 29.7ms
Speed: 1.4ms preprocess, 29.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 576)

image 1/1 /home/xcl/datasets/face_detect/e.png: 192x640 95 faces, 30.2ms
Speed: 0.6ms preprocess, 30.2ms inference, 0.9ms postprocess per image at shape (1, 3, 192, 640)

image 1/1 /home/xcl/datasets/face_detect/f.png: 576x640 3 faces, 30.4ms
Speed: 1.4ms preprocess, 30.4ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 640)
模型预测时间：3.0392
'''

'''
Using CPU for inference

image 1/1 /home/xcl/datasets/face_detect/a.jpg: 384x640 6 faces, 131.0ms
Speed: 9.3ms preprocess, 131.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)

image 1/1 /home/xcl/datasets/face_detect/b.jpg: 384x640 13 faces, 118.3ms
Speed: 2.9ms preprocess, 118.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)

image 1/1 /home/xcl/datasets/face_detect/c.png: 512x640 25 faces, 149.9ms
Speed: 1.9ms preprocess, 149.9ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)

image 1/1 /home/xcl/datasets/face_detect/d.png: 640x576 24 faces, 161.8ms
Speed: 2.4ms preprocess, 161.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 576)

image 1/1 /home/xcl/datasets/face_detect/e.png: 192x640 95 faces, 99.7ms
Speed: 1.0ms preprocess, 99.7ms inference, 0.8ms postprocess per image at shape (1, 3, 192, 640)

image 1/1 /home/xcl/datasets/face_detect/f.png: 576x640 3 faces, 155.6ms
Speed: 2.3ms preprocess, 155.6ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)
模型预测时间：2.8298
'''
```
<br/>
&emsp;&emsp;发现GPU（A100-40G）甚至比CPU（AMD Epyc 7453）要慢，要花时间一些，首先怀疑是否是模型加载花时间，于是在第一张图片推理前进行计时，
结果确实GPU的总计时变快了，但也和CPU不相上下。第二步，我看到GPU推理的第一张图postprocess远远大于其它图，且CPU没这种情况，
且模型对推理的时间只算了inference，预处理和后处理没有加入计算，对于GPU推理第一张图的postprocess时长异常进行进一步的研究讨论。  
在yolov5的代码中，可以找到一个函数叫做warmup(imgsz=(1, 3, 640, 640)),该函数的目的：通过进行一次推理来预热模型，从而提高后续推理的效率。
(源解释：这段 warmup 方法的主要功能是确保在模型进行实际推理之前，先通过一次（或多次）推理来“预热”模型。这样可以提升后续推理的效能，
特别是在模型转移到特定硬件（如 GPU）后。通过这种方式，可以避免初始化阶段的延迟，在正式使用模型时实现更好的性能。
该方法适用于多种模型格式，包括 PyTorch、TorchScript、ONNX、TensorRT等，确保在非 CPU 设备中尽可能地提前配置和优化计算。)

<br/>
<br/>
&emsp;&emsp;在5月份离职了，总结这段实习经理，不得不说遇到了很棒的同事，我的两位mentor(虽然不是领导设定的，但对我的指导与mentor无异)
：秋城落叶、~~脚本小子~~(某高级工程师)，一方面在学生到职员身份的转变上、一方面在终身学习上对我的帮助极大，如何~~摸鱼~~拆分和汇报自己的工作、
如何学习一门和工作无关的技术，让我在下一段西门子实习中可以快的适应下来。  
&emsp;&emsp;回望我的求学生涯似乎一直都在收到周围人的各种帮助，考研的时候女朋友在监督我(虽然当时还不是，但很幸运，我们在今年5月3日确定关系了)、
读研有课题组同门师兄姐妹在监督我的论文进度、第一段研究所实习来自我大佬朋友的内推、第二段算法实习有秋城落叶+脚本小子两个人手把手带入职场、
第三段西门子实习遇到了川轻化工(川理学院)的老师兄分享了不少毕业后的经验，感恩~  
&emsp;&emsp;我也在想我为大家做些什么？但我现在这么菜，好像不太能帮得上，哈哈，所以努力提升自己吧!




